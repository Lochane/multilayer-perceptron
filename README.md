# multilayer-perceptron

# Concepts fondamentaux d‚Äôun r√©seau de neurones

Cette section explique les notions essentielles pour comprendre et coder un r√©seau de neurones, que ce soit **from scratch** ou avec des frameworks comme PyTorch ou TensorFlow.

---

## 1. Poids (Weights)

* **D√©finition :** Coefficient qui mesure l‚Äôimportance d‚Äôune entr√©e (feature) pour un neurone.
* **R√¥le :** Plus le poids est grand, plus la feature influence la sortie du neurone.

* **Formule math√©matique :**
z = w1*x1 + w2*x2 + ... + wn*xn + b


---

## 2. Biais (Bias)

* **D√©finition :** Constante ajout√©e au neurone pour d√©caler la fonction d‚Äôactivation.
* **R√¥le :** Permet au neurone de produire une sortie non nulle m√™me si toutes les entr√©es sont nulles.

* **Formule :**
z = w1*x1 + w2*x2 + ... + wn*xn + b

---

## 3. Fonction d‚Äôactivation (Activation Function)

* **D√©finition :** Transforme la sortie lin√©aire (z = w \cdot x + b) en une valeur non-lin√©aire.
* **Importance :** Sans activation, plusieurs couches √©quivaudraient √† une seule couche (incapable de mod√©liser des relations complexes).

* **Exemples courants :**

| Fonction | Formule                                    | Intervalle               |
| -------- | ------------------------------------------ | ------------------------ |
| Sigmo√Øde | `sigma(z) = 1 / (1 + exp(-z))`             | (0,1)                    |
| ReLU     | `ReLU(z) = max(0, z)`                      | [0,‚àû)                    |
| Softmax  | `softmax(z_i) = exp(z_i) / sum_j exp(z_j)` | somme = 1 (probabilit√©s) |


---

## 4. Gradient

* **D√©finition :** D√©riv√©e de la loss par rapport aux poids ou biais.
* **R√¥le :** Indique **comment ajuster les poids et biais pour r√©duire l‚Äôerreur**.

* **Formule pour un neurone sigmo√Øde avec loss L :**

dL/dw_i = (a - y) * sigma'(z) * x_i
dL/db   = (a - y) * sigma'(z)

or 

sigma'(z) = sigma(z) * (1 - sigma(z))


---

## 5. Concepts math√©matiques essentiels

1. **Multiplication matricielle / vecteurs**
   a = f(Wx + b)

* (W) = matrice des poids
* (b) = vecteur des biais
* (f) = fonction d‚Äôactivation

2. **Fonction de loss**

* Mesure l‚Äôerreur du mod√®le
* Exemples :

  * Classification binaire : cross-entropy
  * Classification multi-classes : categorical cross-entropy
  * R√©gression : mean squared error

3. **Gradient descent**

* Met √† jour les poids pour minimiser la loss :
* w = w - lr * dL/dw

* lr = learning rate

4. **Backpropagation**

* Calcul des gradients couche par couche pour ajuster tous les poids et biais

5. **Non-lin√©arit√©**

* Essentielle pour permettre aux r√©seaux profonds de mod√©liser des relations complexes

---

### üí° R√©sum√© mn√©motechnique

* **Poids** = combien chaque entr√©e compte
* **Biais** = o√π placer la limite de d√©cision
* **Activation** = transformation non lin√©aire de la sortie
* **Gradient** = direction pour corriger les erreurs

---

